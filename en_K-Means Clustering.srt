0
00:00:00,580 --> 00:00:04,970
Hello, and welcome! In this video, we’ll be covering k-Means

1
00:00:04,970 --> 00:00:08,590
clustering. So let’s get started.

2
00:00:08,590 --> 00:00:13,380
Imagine that you have a customer dataset, and you need to apply customer segmentation

3
00:00:13,380 --> 00:00:19,410
on this historical data. Customer segmentation is the practice of partitioning

4
00:00:19,410 --> 00:00:24,460
a customer base into groups of individuals that have similar characteristics.

5
00:00:24,460 --> 00:00:31,920
One of the algorithms that can be used for customer segmentation is k-Means clustering.

6
00:00:31,920 --> 00:00:37,580
k-Means can group data only “unsupervised,” based on the similarity of customers to each other.

7
00:00:37,580 --> 00:00:41,580
Let’s define this technique more formally.

8
00:00:41,580 --> 00:00:49,360
There are various types of clustering algorithms, such as partitioning, hierarchical, or density-based

9
00:00:49,360 --> 00:00:54,100
clustering. k-Means is a type of partitioning clustering,

10
00:00:54,100 --> 00:01:01,560
that is, it divides the data into k non-overlapping subsets (or clusters) without any cluster-internal

11
00:01:01,560 --> 00:01:07,680
structure, or labels. This means, it’s an unsupervised algorithm.

12
00:01:07,680 --> 00:01:12,700
Objects within a cluster are very similar and objects across different clusters are

13
00:01:12,700 --> 00:01:18,500
very different or dissimilar. As you can see, for using k-Means, we have

14
00:01:18,500 --> 00:01:23,310
to find similar samples (for example, similar customers).

15
00:01:23,310 --> 00:01:28,780
Now we face a couple of key questions. First, “How can we find the similarity of samples

16
00:01:28,780 --> 00:01:34,730
in clustering?” And then, “How do we measure how similar two customers are with regard

17
00:01:34,730 --> 00:01:37,150
to their demographics?”

18
00:01:37,150 --> 00:01:41,850
Though the objective of k-Means is to form clusters in such a way that similar samples

19
00:01:41,850 --> 00:01:47,600
go into a cluster, and dissimilar samples fall into different clusters, it can be shown

20
00:01:47,600 --> 00:01:52,670
that instead of a similarity metric, we can use dissimilarity metrics.

21
00:01:52,670 --> 00:01:58,070
In other words, conventionally, the distance of samples from each other is used to shape

22
00:01:58,070 --> 00:02:03,190
the clusters. So, we can say, k-Means tries to minimize

23
00:02:03,190 --> 00:02:08,720
the “intra-cluster” distances and maximize the “inter-cluster” distances.

24
00:02:08,720 --> 00:02:15,760
Now, the question is, “How we can calculate the dissimilarity or distance of two cases,

25
00:02:15,760 --> 00:02:18,560
such as two customers?”

26
00:02:18,560 --> 00:02:23,420
Assume that we have two customers, we’ll call them customer 1 and 2.

27
00:02:23,420 --> 00:02:28,510
Let’s also assume that we have only one feature for each of these two customers, and

28
00:02:28,510 --> 00:02:33,440
that feature is Age. We can easily use a specific type of Minkowski

29
00:02:33,440 --> 00:02:39,670
distance to calculate the distance of these two customers. Indeed, it is the Euclidian

30
00:02:39,670 --> 00:02:45,720
distance. Distance of x1 from x2 is root of 34 minus

31
00:02:45,720 --> 00:02:51,360
30 power 2, which is 4. What about if we have more than one feature,

32
00:02:51,360 --> 00:02:54,569
for example Age and Income?

33
00:02:54,569 --> 00:03:00,930
For example, if we have income and age for each customer, we can still use the same formula,

34
00:03:00,930 --> 00:03:04,280
but this time in a 2-dimensional space.

35
00:03:04,280 --> 00:03:09,770
Also, we can use the same distance matrix for multi-dimensional vectors.

36
00:03:09,770 --> 00:03:16,020
Of course, we have to normalize our feature set to get the accurate dissimilarity measure.

37
00:03:16,020 --> 00:03:20,630
There are other dissimilarity measures as well that can be used for this purpose, but

38
00:03:20,630 --> 00:03:26,800
it is highly dependent on data type and also the domain that clustering is done for it.

39
00:03:26,800 --> 00:03:33,100
For example, you may use Euclidean distance, cosine similarity, average distance, and so

40
00:03:33,100 --> 00:03:37,180
on. Indeed, the similarity measure highly controls

41
00:03:37,180 --> 00:03:42,270
how the clusters are formed, so it is recommended to understand the domain knowledge of your

42
00:03:42,270 --> 00:03:47,950
dataset, and data type of features, and then choose the meaningful distance measurement.

43
00:03:47,950 --> 00:03:54,791
Now, let’s see how k-Means clustering works. For the sake of simplicity, let’s assume

44
00:03:54,791 --> 00:04:00,160
that our dataset has only two features, the age and income of customers.

45
00:04:00,160 --> 00:04:05,760
This means, it’s a 2-dimentional space. We can show the distribution of customers

46
00:04:05,760 --> 00:04:12,190
using a scatterplot. The y-axes indicates Age and the x-axes shows

47
00:04:12,190 --> 00:04:17,209
Income of customers. We try to cluster the customer dataset into

48
00:04:17,209 --> 00:04:22,800
distinct groups (or clusters) based on these two dimensions.

49
00:04:22,800 --> 00:04:27,080
In the first step, we should determine the number of clusters.

50
00:04:27,080 --> 00:04:32,330
The key concept of the k-Means algorithm is that it randomly picks a center point for

51
00:04:32,330 --> 00:04:37,669
each cluster. It means, we must initialize k, which represents

52
00:04:37,669 --> 00:04:42,580
"number of clusters." Essentially, determining the number of clusters

53
00:04:42,580 --> 00:04:49,889
in a data set, or k, is a hard problem in k-Means that we will discuss later.

54
00:04:49,889 --> 00:04:55,400
For now, let’s put k equals 3 here, for our sample dataset.

55
00:04:55,400 --> 00:05:00,560
It is like we have 3 representative points for our clusters.

56
00:05:00,560 --> 00:05:06,490
These 3 data points are called centroids of clusters, and should be of same feature size

57
00:05:06,490 --> 00:05:13,610
of our customer feature set. There are two approaches to choose these centroids:

58
00:05:13,610 --> 00:05:20,029
1) We can randomly choose 3 observations out of the dataset and use these observations

59
00:05:20,029 --> 00:05:26,309
as the initial means. Or, 2) We can create 3 random points as centroids

60
00:05:26,309 --> 00:05:32,559
of the clusters, which is our choice that is shown in this plot with red color.

61
00:05:32,559 --> 00:05:38,669
After the initialization step, which was defining the centroid of each cluster, we have to assign

62
00:05:38,669 --> 00:05:44,500
each customer to the closest center. For this purpose, we have to calculate the

63
00:05:44,500 --> 00:05:51,169
distance of each data point (or in our case, each customer) from the centroid points.

64
00:05:51,169 --> 00:05:55,849
As mentioned before, depending on the nature of the data and the purpose for which clustering

65
00:05:55,849 --> 00:06:02,379
is being used, different measures of distance may be used to place items into clusters.

66
00:06:02,379 --> 00:06:07,649
Therefore, you will form a matrix where each row represents the distance of a customer

67
00:06:07,649 --> 00:06:12,990
from each centroid. It is called the "distance-matrix."

68
00:06:12,990 --> 00:06:18,160
The main objective of k-Means clustering is to minimize the distance of data points from

69
00:06:18,160 --> 00:06:24,379
the centroid of its cluster and maximize the distance from other cluster centroids.

70
00:06:24,379 --> 00:06:30,400
So, in this step we have to find the closest centroid to each data point.

71
00:06:30,400 --> 00:06:36,199
We can use the distance-matrix to find the nearest centroid to data points.

72
00:06:36,199 --> 00:06:42,310
Finding the closest centroids for each data point, we assign each data point to that cluster.

73
00:06:42,310 --> 00:06:48,039
In other words, all the customers will fall to a cluster, based on their distance from

74
00:06:48,039 --> 00:06:51,789
centroids. We can easily say that it does not result

75
00:06:51,789 --> 00:06:56,490
in good clusters, because the centroids were chosen randomly from the first.

76
00:06:56,490 --> 00:07:02,750
Indeed, the model would have a high error. Here, error is the total distance of each

77
00:07:02,750 --> 00:07:08,389
point from its centroid. It can be shown as within-cluster sum of squares

78
00:07:08,389 --> 00:07:12,929
error. Intuitively, we try to reduce this error.

79
00:07:12,929 --> 00:07:18,050
It means we should shape clusters in such a way that the total distance of all members

80
00:07:18,050 --> 00:07:24,679
of a cluster from its centroid be minimized. Now, the question is, "How we can turn it

81
00:07:24,679 --> 00:07:27,960
into better clusters, with less error?"

82
00:07:27,960 --> 00:07:34,210
Okay, we move centroids. In the next step, each cluster center will

83
00:07:34,210 --> 00:07:40,679
be updated to be the mean for data points in its cluster. Indeed, each centroid moves

84
00:07:40,679 --> 00:07:45,969
according to their cluster members. In other words, the centroid of each of the

85
00:07:45,969 --> 00:07:53,340
3 clusters becomes the new mean. For example, if Point A coordination is 7.4

86
00:07:53,340 --> 00:08:01,949
and 3.6, and Point B features are 7.8 and 3.8, the new centroid of this cluster with

87
00:08:01,949 --> 00:08:08,699
2 points, would be the average of them, which is 7.6 and 3.7.

88
00:08:08,699 --> 00:08:14,619
Now we have new centroids. As you can guess, once again, we will have

89
00:08:14,619 --> 00:08:19,309
to calculate the distance of all points from the new centroids.

90
00:08:19,309 --> 00:08:23,270
The points are re-clustered and the centroids move again.

91
00:08:23,270 --> 00:08:27,159
This continues until the centroids no longer move.

92
00:08:27,159 --> 00:08:32,520
Please note that whenever a centroid moves, each point’s distance to the centroid needs

93
00:08:32,520 --> 00:08:34,310
to be measured again.

94
00:08:34,310 --> 00:08:42,190
Yes, k-Means is an iterative algorithm, and we have to repeat steps 2 to 4 until the algorithm

95
00:08:42,190 --> 00:08:47,450
converges. In each iteration, it will move the centroids,

96
00:08:47,450 --> 00:08:52,510
calculate the distances from new centroids, and assign the data points to the nearest

97
00:08:52,510 --> 00:08:56,470
centroid. It results in the clusters with minimum error,

98
00:08:56,470 --> 00:09:02,551
or the most dense clusters. However, as it is a heuristic algorithm, there

99
00:09:02,551 --> 00:09:07,550
is no guarantee that it will converge to the global optimum, and the result may depend

100
00:09:07,550 --> 00:09:12,810
on the initial clusters. It means this algorithm is guaranteed to converge

101
00:09:12,810 --> 00:09:19,740
to a result, but the result may be a local optimum (i.e. not necessarily the best possible

102
00:09:19,740 --> 00:09:23,800
outcome). To solve this problem, it is common to run

103
00:09:23,800 --> 00:09:28,910
the whole process, multiple times, with different starting conditions.

104
00:09:28,910 --> 00:09:34,260
This means, with randomized starting centroids, it may give a better outcome.

105
00:09:34,260 --> 00:09:39,860
And as the algorithm is usually very fast, it wouldn’t be any problem to run it multiple times.

106
00:09:41,640 --> 00:09:43,050
Thanks for watching this video!

